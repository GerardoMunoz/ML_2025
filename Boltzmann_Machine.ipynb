{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmq6ZNJRsjJx/aEZiS4k0u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardoMunoz/ML_2025/blob/main/Boltzmann_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Boltzmann Machine: A Stochastic Hopfield Network**\n",
        "\n",
        "## **Introduction**\n",
        "A Boltzmann Machine is a type of stochastic recurrent neural network similar to a Hopfield Network, but with two major differences:\n",
        "\n",
        "1. **Hidden Neurons** – Unlike Hopfield Networks, which consist only of visible (output) neurons, BMs introduce hidden neurons to model complex data distributions.  \n",
        "2. **Learning Algorithm** – Instead of using a deterministic weight update rule like Hopfield Networks, BMs learn using **stochastic gradient descent (SGD) and Gibbs sampling**.  \n",
        "\n",
        "\n",
        "\n",
        "## **Network Structure**\n",
        "\n",
        "- **Visible** neurons ($ V_i $) representing observed data.\n",
        "- **Hidden** neurons ($ H_i $) capturing latent features.\n",
        "\n",
        "The neurons are fully connected (except for self-connections). The connections have weights that determine the probability of states.\n",
        "\n",
        "## **Energy Function**\n",
        "The energy of a given state **\\( s \\)** in a Boltzmann Machine is defined as:  \n",
        "\n",
        "$$\n",
        "E(s) = -\\sum_{i<j} W_{ij} s_i s_j - \\sum_i b_i s_i\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ W_{ij} $ are the weights between neurons,\n",
        "- $ s_i $ is the state (0 or 1) of neuron $ i $,\n",
        "- $ b_i $ is the bias term.\n",
        "\n",
        "## **Training via Gibbs Sampling**\n",
        "- Initialize neurons with random states.\n",
        "- Compute probability of flipping each neuron using the **sigmoid function**:\n",
        "\n",
        "  $$\n",
        "  P(s_i = 1) = \\frac{1}{1 + e^{- (\\sum_j W_{ij} s_j + b_i )}}\n",
        "  $$\n",
        "\n",
        "- Repeat until convergence.\n",
        "\n",
        "## **Probabilistic Expectation in BM Weight Updates**  \n",
        "In a **Boltzmann Machine (BM)**, weights are updated using the expectation over the probability distribution of the states:\n",
        "\n",
        "$$\n",
        "\\Delta W_{ij} = \\eta \\left( \\langle s_i s_j \\rangle_{\\text{data}} - \\langle s_i s_j \\rangle_{\\text{model}} \\right)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $ \\langle s_i s_j \\rangle_{\\text{data}} $ is the expectation **under the data distribution**.\n",
        "- $ \\langle s_i s_j \\rangle_{\\text{model}} $ is the expectation **under the model's learned distribution**.\n",
        "\n",
        "Instead of directly using $ s_i s_j $, we compute expectations over **probabilistic activations**:\n",
        "\n",
        "$$\n",
        "\\langle s_i s_j \\rangle_{\\text{data}} = \\sum_{s} P_{\\text{data}}(s) \\cdot s_i s_j\n",
        "$$\n",
        "\n",
        "where $ P_{\\text{data}}(s) $ represents the probability of a particular state $ s $ occurring in the data.  \n",
        "The model tries to match these expectations using **Gibbs Sampling** over all possible states.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# **Restricted Boltzmann Machine (RBM): A Simplified Boltzmann Machine**\n",
        "\n",
        "## **RBM vs. Boltzmann Machine**\n",
        "RBMs are a special type of Boltzmann Machine with one key restriction:  \n",
        " **No intra-layer connections** – meaning there are **no connections between hidden neurons** and **no connections between visible neurons**.  \n",
        "\n",
        "This restriction makes training **much more efficient** compared to general BMs.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **RBM Structure & Energy Function**\n",
        "### **Network Structure**\n",
        "- **Visible Layer** ($ V $): Represents input data (e.g., pixels in an image).\n",
        "- **Hidden Layer** ($ H $): Captures latent features.\n",
        "- **Weights** ($ W $): Connect visible and hidden layers, but **no connections within each layer**.\n",
        "\n",
        "### **Energy Function**  \n",
        "(Same idea as BMs but simplified due to no intra-layer connections):\n",
        "\n",
        "$$\n",
        "E(V, H) = - \\sum_{i,j} V_i W_{ij} H_j - \\sum_i b_i V_i - \\sum_j c_j H_j\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ W_{ij} $ is the weight between visible neuron $ V_i $ and hidden neuron $ H_j $,\n",
        "- $ b_i $ and $ c_j $ are biases for visible and hidden layers.\n",
        "\n",
        "### **Training: Contrastive Divergence (CD)**\n",
        "Instead of full **Gibbs Sampling**, RBMs use **Contrastive Divergence (CD)** for faster training:\n",
        "1. **Forward Pass:** Compute hidden activations given the visible layer.\n",
        "2. **Reconstruct Visible Layer:** Use hidden activations to generate new visible layer values.\n",
        "3. **Update Weights:** Compute weight differences between the original and reconstructed states.\n",
        "\n",
        "---\n",
        "\n",
        "## **Probabilistic Activation in RBM Training**  \n",
        "In an **RBM**, the bipartite structure makes the conditional probabilities **independent**, simplifying the weight updates. The update rule is:\n",
        "\n",
        "$$\n",
        "\\Delta W_{ij} = \\eta \\left( P(H_j = 1 | V) V_i - P(H_j = 1 | V') V'_i \\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ P(H_j = 1 | V) $ is the probability of hidden unit \\( H_j \\) being active, given visible units:\n",
        "  \n",
        "  $$\n",
        "  P(H_j = 1 | V) = \\sigma \\left( \\sum_i V_i W_{ij} + c_j \\right)\n",
        "  $$\n",
        "\n",
        "- $ P(V_i = 1 | H) $ is the probability of visible unit \\( V_i \\) being active, given hidden units:\n",
        "  \n",
        "  $$\n",
        "  P(V_i = 1 | H) = \\sigma \\left( \\sum_j H_j W_{ij} + b_i \\right)\n",
        "  $$\n",
        "\n",
        "Here, the weight update **does not use raw activations** but rather the **expected probability** of activation under the data and model distributions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BFQd3KGOlFcc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kYPEkkTlvjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACVVsK-gk-NR"
      },
      "outputs": [],
      "source": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8MC/MqKSO2bTgycMz9u7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardoMunoz/ML_2025/blob/main/Hopfield_Covariance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A80Zj8otb-CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **Linear Algebra Topics**\n",
        "\n",
        "---\n",
        "\n",
        "## **Dot Product**\n",
        "\n",
        "The dot product of two vectors $u$ and $v$ is defined as:\n",
        "\n",
        "$$ u \\cdot v = \\sum_{i=1}^{n} u_i v_i $$\n",
        "\n",
        "Alternatively, using matrix notation:\n",
        "\n",
        "$$ u \\cdot v = u^T v $$\n",
        "\n",
        "\n",
        "**Example: Dot Product in 3D**\n",
        "\n",
        "Given two vectors in 3D:\n",
        "\n",
        "$$ u = (2, -1, 3), \\quad v = (4, 0, -2) $$\n",
        "\n",
        "The dot product is computed as:\n",
        "\n",
        "$$ u \\cdot v = (2)(4) + (-1)(0) + (3)(-2) $$\n",
        "\n",
        "$$ = 8 + 0 - 6 = 2 $$\n",
        "\n",
        "Thus, the dot product of $u$ and $v$ is **2**.\n",
        "\n",
        "**Properties:**\n",
        "- Commutative: \\( u \\cdot v = v \\cdot u \\)\n",
        "- Distributive: \\( u \\cdot (v + w) = u \\cdot v + u \\cdot w \\)\n",
        "- Scalar multiplication: \\( (cu) \\cdot v = c (u \\cdot v) \\)\n",
        "\n",
        "---\n",
        "\n",
        "## **Magnitude of a Vector**\n",
        "\n",
        "The magnitude (or norm) of a vector $u$ is given by:\n",
        "\n",
        "$$ ||u|| = \\sqrt{u \\cdot u} = \\sqrt{\\sum_{i=1}^{n} u_i^2} $$\n",
        "\n",
        "In 3D, for a vector $u$:\n",
        "\n",
        "$$ u = (x, y, z) $$\n",
        "\n",
        "The magnitude is:\n",
        "\n",
        "$$ ||u|| = \\sqrt{x^2 + y^2 + z^2} $$\n",
        "\n",
        "**Example in 3D**\n",
        "\n",
        "Given the vector:\n",
        "\n",
        "$$ u = (3, -4, 12) $$\n",
        "\n",
        "The magnitude is calculated as:\n",
        "\n",
        "$$ ||u|| = \\sqrt{3^2 + (-4)^2 + 12^2} $$\n",
        "\n",
        "$$ = \\sqrt{9 + 16 + 144} $$\n",
        "\n",
        "$$ = \\sqrt{169} = 13 $$\n",
        "\n",
        "Thus, the magnitude of $u$ is **13**.\n",
        "\n",
        "**Properties:**\n",
        "- **Non-negative:** The magnitude of any vector is always $ \\geq 0 $.\n",
        "- **Zero Vector:** $ ||u|| = 0 $ if and only if $ u $ is the zero vector.\n",
        "- **Homogeneity (Scaling):** For any scalar $ c $, $ ||c u|| = |c| ||u|| $.\n",
        "- **Triangle Inequality:** $ ||u + v|| \\leq ||u|| + ||v|| $.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## **Projection in 1D**\n",
        "\n",
        "To project a vector $u$ onto a  vector $v$:\n",
        "\n",
        "\n",
        "$$ \\text{proj}_v u = \\frac{u \\cdot v}{v \\cdot v} v $$\n",
        "\n",
        "This gives the closest point to $u$ on the line that spans $v$\n",
        "\n",
        "---\n",
        "\n",
        "## **Pseudo Inverse**\n",
        "\n",
        "For a matrix **A**, the Moore-Penrose pseudo-inverse \\( A^+ \\) is defined as:\n",
        "\n",
        "$$ A^+ = (A^T A)^{-1} A^T $$\n",
        "\n",
        "When **A** is not square or not invertible, this gives the best least-squares solution to \\( Ax = b \\).\n",
        "\n",
        "**Properties:**\n",
        "- \\( A A^+ A = A \\)\n",
        "- \\( A^+ A A^+ = A^+ \\)\n",
        "- Symmetry in transposition: \\( (A^+)^T = (A^T)^+ \\)\n",
        "\n"
      ],
      "metadata": {
        "id": "kk3o96vL4Gq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **Vector Distances and Similarities**\n",
        "If the operation satisfies these properties it is called **distance**.\n",
        "\n",
        "- Non-negative: \\( d(u, v) \\geq 0 \\)\n",
        "- Identity: \\( d(u, v) = 0 \\iff u = v \\)\n",
        "- Symmetry: \\( d(u, v) = d(v, u) \\)\n",
        "- Triangle inequality: \\( d(u, w) \\leq d(u, v) + d(v, w) \\)\n",
        "\n",
        "But if the operation that allows comparing vectors does not satisfy all the properties is called **similarity**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Euclidean Distance**\n",
        "\n",
        "The Euclidean distance between two vectors **u** and **v** is given by:\n",
        "\n",
        "$$ d(u, v) = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2} $$\n",
        "\n",
        "Alternatively, in vector notation:\n",
        "\n",
        "$$ d(u, v) = || u - v || $$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Manhattan Distance**\n",
        "\n",
        "Also called **L1 norm**, the Manhattan distance is:\n",
        "\n",
        "$$ d_1(u, v) = \\sum_{i=1}^{n} |u_i - v_i| $$\n",
        "\n",
        "This measures the total absolute difference across dimensions.\n",
        "\n",
        "**Best for:**\n",
        "- Grid-based distance calculations (e.g., city block distance)\n",
        "- Sparse or high-dimensional data\n",
        "\n",
        "---\n",
        "\n",
        "## **Cosine Similarity**\n",
        "\n",
        "Measures the angle between vectors:\n",
        "\n",
        "$$ \\text{cosine}(u, v) = \\frac{u \\cdot v}{||u|| ||v||} $$\n",
        "\n",
        "**Range:**\n",
        "- $ -1 $ (opposite direction)\n",
        "- $ 0 $ (orthogonal)\n",
        "- $ 1 $ (same direction)\n",
        "\n",
        "Useful in:\n",
        "- Text analysis (TF-IDF vectors)\n",
        "- Recommender systems\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Minkowski Distance**\n",
        "\n",
        "The generalized form of Euclidean and Manhattan distances:\n",
        "\n",
        "$$ d_p(u, v) = \\left( \\sum_{i=1}^{n} |u_i - v_i|^p \\right)^{\\frac{1}{p}} $$\n",
        "\n",
        "- $ p = 1 $ → Manhattan Distance\n",
        "- $ p = 2 $ → Euclidean Distance\n",
        "- $ p \\to \\infty $ → Chebyshev Distance (max difference along any coordinate)\n",
        "\n",
        "---\n",
        "\n",
        "## **Hamming Distance**\n",
        "\n",
        "Counts differing elements:\n",
        "\n",
        "$$ d_H(u, v) = \\sum_{i=1}^{n} \\mathbf{1}(u_i \\neq v_i) $$\n",
        "\n",
        "Used for:\n",
        "- Comparing binary strings (e.g., error detection)\n",
        "- DNA sequencing\n",
        "\n",
        "---\n",
        "\n",
        "# **Applications of Distances & Similarities**\n",
        "\n",
        "- **Machine Learning**: Nearest Neighbor classification, clustering\n",
        "- **Natural Language Processing**: Text similarity, document comparison\n",
        "- **Image Processing**: Feature matching, object recognition\n",
        "- **Graph Theory**: Network analysis, social connections\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mVjxULcJ1NRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gram matrix: Covariance Matrix and Hopfield Network**\n",
        "\n",
        "### Introduction\n",
        "Given this two vectors in $\\mathbb{R}^3$\n",
        "\n",
        "$$\n",
        "v_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ -1 \\end{bmatrix}, \\quad\n",
        "v_2 = \\begin{bmatrix} -1 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can define a transformation from $\\mathbb{R}^2$ to $\\mathbb{R}^3$ using the matriz $A=[v_1\\ \\ v_2]$:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "y_1\\\\y_2\\\\y_3\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0.5 & -1 \\\\\n",
        "0.5 & 0.5 \\\\\n",
        "-1 & 0.5\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\x_2\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "It is also possible to define a transformation from $\\mathbb{R}^3$ to $\\mathbb{R}^2$ using $A^T$\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x'_1\\\\x'_2\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0.5 & 0.5 & -1 \\\\\n",
        "-1 & 0.5 & 0.5\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "y'_1\\\\y'_2\\\\y'_3\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here we will study the composition of both $A^TA$ and $AA^T$ which are called **Gram matrices**.\n",
        "\n",
        "\n",
        "\n",
        "### Gram matrix is a matrix of dot products.\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A^T = \\begin{bmatrix} v_1^T \\\\ v_2^T \\\\ \\vdots \\\\ v_n^T \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A^TA = \\begin{bmatrix} v_1^T \\\\ v_2^T \\\\ \\vdots \\\\ v_n^T \\end{bmatrix}\\begin{bmatrix} v_1 & v_2 & \\cdots & v_3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A^TA = \\begin{bmatrix}\n",
        "v_1 \\cdot v_1 & v_1 \\cdot v_2 & \\cdots & v_1 \\cdot v_n\\\\\n",
        "v_2 \\cdot v_1& v_2 \\cdot v_2  & \\cdots & v_2 \\cdot v_n\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots &\\\\\n",
        "v_n \\cdot v_1& v_n \\cdot v_2  & \\cdots & v_n \\cdot v_n \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- **Size**: If $A$ is $ m \\times n $ matrix  (i.e. each vector $v_i$ has $m$ rows) then $A^TA$ is $ n \\times n $ square matrix. And $AA^T$ is $ m \\times m $ square matrix. It represents different things if $m>n$ or if $m<n$.\n",
        "- **Diagonal values**: $ G_{ii} = v_i \\cdot v_i $ → **squared norm** of each vector.  \n",
        "- **Off-diagonal values**: $ G_{ij} = v_i \\cdot v_j $ → **similarity** between vectors.  \n",
        "\n",
        "$A^TA$ is used in the Moore-Penrose pseudo-inverse. But in the next two sections we are going to use $AA^T$, where the matrix is square $m \\times m$ and $m$ is the number of components of each vector $v_i$.\n"
      ],
      "metadata": {
        "id": "pIaFI8_ZzPpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Hopfield Network**\n",
        "**More dimensions than vectors**. So if A is $ m \\times n$ then $m>n$.\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Each vector $v_i$ is a pattern that will be stored in the  Hopfield Network  **weight matrix** ($W$), computed as:\n",
        "\n",
        "$$\n",
        "W =  AA^T\n",
        "$$\n",
        "\n",
        "In our example, it is a  transformation from $\\mathbb{R}^3$ to $\\mathbb{R}^3$.\n",
        "If\n",
        "$$\n",
        "A=\\begin{bmatrix}\n",
        "0.5 & -1 \\\\\n",
        "0.5 & 0.5 \\\\\n",
        "-1 & 0.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "then\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "y_1\\\\y_2\\\\y_3\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0.5 & -1 \\\\\n",
        "0.5 & 0.5 \\\\\n",
        "-1 & 0.5\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.5 & 0.5 & -1 \\\\\n",
        "-1 & 0.5 & 0.5\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "y'_1\\\\y'_2\\\\y'_3\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "y_1\\\\y_2\\\\y_3\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}1.25 & -0.25 & -1.0\\\\-0.25 & 0.5 & -0.25\\\\-1.0 & -0.25 & 1.25\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "y'_1\\\\y'_2\\\\y'_3\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The weights $W$ allow us to recover the patterns of positive (+1) and negative (-1) values, where each column of $A$ is one pattern.\n",
        "\n",
        "The first column is\n",
        "\n",
        "$$\n",
        "v_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ -1 \\end{bmatrix},\n",
        "$$\n",
        "\n",
        "So it has a pattern of positives and negatives.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix},\n",
        "$$\n",
        "\n",
        "If we add noise, for example to the second row, it sign changes.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\end{bmatrix},\n",
        "$$\n",
        "\n",
        "Now we use $W$ to restore the pattern.\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}1.25 & -0.25 & -1.0\\\\-0.25 & 0.5 & -0.25\\\\-1.0 & -0.25 & 1.25\\end{bmatrix}\n",
        "\\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\n",
        "=\\begin{bmatrix}0.75\\\\0.75\\\\-1.5\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "So the answer recovers the pattern.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix},\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4nKWoWFb0ULC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **Covariance Matrix**\n",
        "**More vectors than dimensions**.  So if A is $ m \\times n$ then $m<n$.\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "In statistics, the **covariance matrix** is:\n",
        "\n",
        "$$\n",
        "C = AA^T\n",
        "$$\n",
        "\n",
        "In our example it is a transformation from $\\mathbb{R}^2$ to $\\mathbb{R}^2$.\n",
        "If\n",
        "$$\n",
        "A=\\begin{bmatrix}\n",
        "0.5 & 0.5 & -1 \\\\\n",
        "-1 & 0.5 & 0.5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Then\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x'_1\\\\x'_2\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "0.5 & 0.5 & -1 \\\\\n",
        "-1 & 0.5 & 0.5\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.5 & -1 \\\\\n",
        "0.5 & 0.5 \\\\\n",
        "-1 & 0.5\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\x_2\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x'_1\\\\x'_2\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}1.5 & -0.75\\\\-0.75 & 1.5\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\x_2\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9l4dxXGUTgCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Viewing the transpose of $A$\n",
        "\n",
        "Suppose that all the vectors of the matrix are in $\\mathbb{R}^2$, and that the sum of all vectors is the vector zero. So we can write\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} v_{1x} & v_{2x} & \\cdots & v_{nx} \\\\ v_{1y} & v_{2y} & \\cdots & v_{ny} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The transpose is\n",
        "\n",
        "$$\n",
        "B=A^T=\\begin{bmatrix} v_{1x} & v_{1y} \\\\ v_{2x} & v_{2y} \\\\ \\vdots & \\vdots \\\\ v_{nx} & v_{ny} \\end{bmatrix} = \\begin{bmatrix} u_{1} & u_{2}  \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "So now we have only two vectors, the first one has all the $x$ components, and the second one has all the $y$ components. Now we are going to calculate the covariance matrix and interpret it. Remember that $B=A^T$ and so $A=B^T$.\n",
        "\n",
        "$$\n",
        "C=AA^T=B^TB =  \\begin{bmatrix} u_{1}^T \\\\ u_{2}^T  \\end{bmatrix} \\begin{bmatrix} u_{1} & u_{2}  \\end{bmatrix} = \\begin{bmatrix} u_{1} \\cdot u_1 & u_1 \\cdot u_{2} \\\\ u_{2} \\cdot u_1 & u_2 \\cdot u_{2} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The first element is\n",
        "$$u_{1} \\cdot u_1=||u_1||^2$$\n",
        "that is the square of the norm of the vector with all $x$ components. So if the x components are high then the norm is high, so this component represents how the points are distributed along the $x$ axis.  The same will happen with all elements of the diagonal, in particular, the last element\n",
        "$$  u_2 \\cdot u_{2}=||u_2||^2$$\n",
        "represents the distribution of the points along the $y$ axis. This is called the variance of the axis.\n",
        "\n",
        "\n",
        "The second element and the third element are the same because the dot product is commutative. So this matriz is simetric.\n",
        "$$ u_1 \\cdot u_{2} = u_{2} \\cdot u_1$$\n",
        "This dot product is high if both components are large, and this happens if the points are along the line $x=y$. So this component represents how the points are related in both components. In general, the non-diagonal elements represent the relation of both components. This is called the covariance between the two axes.\n",
        "\n",
        "\n",
        "The Covariance Matrix $C$ (or the weight matrix) when multiplying a vector ($Cw$), transforms the components to better match the patterns."
      ],
      "metadata": {
        "id": "di1TGaPa2Bep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "k77yu6hWBE3z",
        "outputId": "7b65bfb7-a656-4ba9-b1bf-cb9a2629ed52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sign(x):\n",
        "    return np.where(x >= 0, 1, -1)\n",
        "\n",
        "def train_hopfield(patterns):\n",
        "    N = patterns.shape[1]\n",
        "    W = np.zeros((N, N))\n",
        "    for p in patterns:\n",
        "        W += np.outer(p, p)\n",
        "    np.fill_diagonal(W, 0)  # No self-connections\n",
        "    return W / N  # Normalize weights\n",
        "\n",
        "def recall_hopfield(W, input_pattern, steps=5):\n",
        "    s = input_pattern.copy()\n",
        "    for _ in range(steps):\n",
        "        s = sign(W @ s)  # Update all neurons at once\n",
        "    return s\n",
        "\n",
        "def display_pattern(pattern, title=\"Pattern\"):\n",
        "    size = int(np.sqrt(len(pattern)))\n",
        "    plt.imshow(pattern.reshape(size, size), cmap=\"gray\")\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Define 3x3 patterns for '/' and '\\'\n",
        "pattern_slash = np.array([\n",
        "    -1, -1,  1,\n",
        "    -1,  1, -1,\n",
        "     1, -1, -1\n",
        "])\n",
        "\n",
        "pattern_backslash = np.array([\n",
        "     1, -1, -1,\n",
        "    -1,  1, -1,\n",
        "    -1, -1,  1\n",
        "])\n",
        "\n",
        "# Store patterns in Hopfield Network\n",
        "patterns = np.array([pattern_slash, pattern_backslash])\n",
        "W = train_hopfield(patterns)\n",
        "\n",
        "# Create a noisy input\n",
        "noisy_input = np.array([\n",
        "    -1, -1, -1,\n",
        "    -1,  1, -1,\n",
        "     1, -1, -1\n",
        "])\n",
        "\n",
        "display_pattern(noisy_input, \"Noisy Input\")\n",
        "\n",
        "# Recall stored pattern\n",
        "recovered_pattern = recall_hopfield(W, noisy_input)\n",
        "display_pattern(recovered_pattern, \"Recovered Pattern\")\n",
        "\n"
      ]
    }
  ]
}